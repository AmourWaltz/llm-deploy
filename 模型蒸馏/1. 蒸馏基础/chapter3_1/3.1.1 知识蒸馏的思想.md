# 知识蒸馏的思想


先训练一个大的网络模型（Teacher Network），再根据该网络模型来训练小的网络模型（Student Network）。教师网络较大且臃肿，可能是多个网络的集成，同时也包含了很多信息。知识蒸馏的具体学习方法是：学生网络将教师网络的输出结果当成正确答案来训练，而网络剪枝主要通过剪掉参数或神经元来压缩网络，这也是网络剪枝与知识蒸馏方法的区别所在。该方法的好处是，教师网络的输出结果对于小网络模型来说更容易学习；一些没有用于学生网络训练而用于教师网络训练的样本依然可以很好地被学生网络识别出来。

<div align=center>
<img src="https://github.com/gyfffffff/llm-deploy/blob/main/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/1.%20%E8%92%B8%E9%A6%8F%E5%9F%BA%E7%A1%80/chapter3_1/images/Figure%202.png" width="700" >
</div>

之所以要进行知识蒸馏，因为大部分人工智能算法都很大，教师网络一般是用海量数据与资源训练出来的，而真正落地运用的终端的算力却非常有限，如智能手表、手机、移动端、IoT设备、无人驾驶汽车等，因此需要进行知识蒸馏。

知识蒸馏的技巧：在softmax函数上加入temperature

Softmax函数：y_i^'=(exp⁡(y_i))/(∑_i▒〖exp⁡(y_i)〗)，该函数将网络的最终输出变成一个介于0到1的概率的分布，加入温度后的softmax函数：y_i^'=(exp⁡(y_i/T))/(∑_i▒〖exp⁡(y_i/T)〗)，T表示温度（temperature），新的函数会使得网络新输出的结果比起原结果更平滑、更接近，学生网络可以学习到更相近的输出值，有利于学生网络的学习。

<div align=center>
<img src="https://github.com/gyfffffff/llm-deploy/blob/main/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/1.%20%E8%92%B8%E9%A6%8F%E5%9F%BA%E7%A1%80/chapter3_1/images/Figure%203.png" width="700" >
</div>
