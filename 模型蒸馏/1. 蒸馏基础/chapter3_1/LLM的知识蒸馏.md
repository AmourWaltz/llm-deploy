# LLM的知识蒸馏
自2017年起，在自然语言处理领域，预训练大模型的参数规模呈指数级别增长，但终端场景的算力比较有限。目前来说，要想调用大模型的功能，需要先把大模型部署到云端或某个数据中心，再通过API远程访问。这种方法会导致网络延迟、依附网络才能应用模型等问题的存在，因此轻量化神经网络具有必要性。模型压缩可以将大型、资源密集型模型转换为适合存储在受限移动设备上的紧凑版本。此外它可以优化模型，以最小的延迟更快地执行，或实现这些目标之间的平衡。
