# 3.3 结构化剪枝

## 介绍

&emsp;&emsp;结构化剪枝是一种用于神经网络模型优化的技术，其目的是通过移除神经网络中的整个结构组件（例如神经元、通道或层），而不是单个的权重或连接。这种方法同时针对整组权重，具有降低模型复杂性和内存使用量，同时保持整体权重的优点。与非结构化剪枝不同，非结构化剪枝是针对单个权重进行剪枝，删除一些权重值较小的连接，而结构化剪枝则是根据特定规则针对整个子结构进行剪枝。结构化剪枝的一个关键优势在于它更适合硬件加速，因为删除整个结构单元比删除单个权重更容易被硬件高效实现。结构化剪枝保持了模型的规则性，这使得模型更容易被硬件优化。例如，某些硬件加速器可能对规则的矩阵运算有更好的支持。

## 相关基础

###  LLMPruner

&emsp;&emsp;LLMPruner是第一个提出LLM结构化剪枝的方法。LLMPruner能够“物理地”移除冗余的结构和参数，同时保留大部分原模型已经学习到的参数，实现高效的大语言模型压缩。&emsp;&emsp;LLMPruner的示例图如下图所示，主要可以概括为：

![](images/llmpruner.png)

- 与任务无关的压缩，其中压缩的语言模型保留其作为多任务求解器的能力。
- 减少了对原始训练语料库的需求，压缩只需要 5 万个公开可用的样本，大大减少了获取训练数据的预算。
- 快速压缩，压缩过程在三个小时内完成。
- 自动结构修剪框架，其中所有依赖结构都被分组，而不需要任何手动设计。

&emsp;&emsp;LLMPruner的算法流程主要分为三个阶段：

![](images/llmpruner-flow.png)

- 发现阶段。此步骤的重点是识别LLMs中相互依赖的结构组。
- 估算阶段。一旦耦合结构被分组，估计每个组对模型整体性能的贡献，并决定要修剪的组。
- 恢复阶段。此步骤涉及快速的再训练，以减轻因移除结构而导致的潜在性能下降。

&emsp;&emsp;**发现阶段。** 鉴于训练后数据的可用性有限，在压缩模型时必须优先删除损坏最小的结构。这强调了基于依赖关系的结构修剪，确保耦合结构被一致修剪。在发现阶段，主要是发现在LLMs中所有的耦合结构。

- 定义了依赖原则：如果一个神经元A依赖另一个神经元B，修剪A则必须修剪B；
- 根据依赖原则，遍历所有神经元，形成依赖关系图。
- 如下图所示，触发神经元标记为带有铃铛的圆圈，导致依赖性修剪的权重（虚线），这些权重可能会传播（红色虚线）到耦合神经元（虚线）。

![](images/Illustration.png)

&emsp;&emsp;**估算阶段。** 经过发现阶段后，模型中的所有耦合结构都已分组。通过评估整个组的重要性，而不是评估模块的重要性，在评估每个组的重要性后，对每个组的重要性进行排序，并根据预定义的修剪比率修剪重要性较低的组。计算组重要性的方法如下：
- 计算梯度（向量）重要性来估计组的重要性；
- 计算参数（每个元素、对角线元素）重要性来估计组的重要性；
- 用四种方式（求和、求积、求最大值、最后一个执行结构）聚合重要性分数：

&emsp;&emsp;**恢复阶段。** 通过Lora对剪枝后的模型进行微调。Lora的原理是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。在涉及到矩阵相乘的模块，在原始的预训练模型旁边增加一个新的通路，通过前后两个矩阵A,B相乘，第一个矩阵A负责降维，第二个矩阵B负责升维。只需在原始预训练模型旁边保存一小部分 LoRA 权重。在推理时，将训练完成的矩阵乘积跟原本预训练模型的权重矩阵W加到一起作为新权重参数替换原本预训练模型的权重W。

![](images/lora.png)


## 参考文献

- [LLM-Pruner: On the Structural Pruning of Large Language Models](https://arxiv.org/abs/2305.11627)
- [知乎NeurIPS 2023 | LLM-Pruner: 大语言模型的结构化剪枝](https://zhuanlan.zhihu.com/p/630902012)