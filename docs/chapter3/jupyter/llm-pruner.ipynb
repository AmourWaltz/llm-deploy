{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Pruner实践\n",
    "> 参考链接：https://github.com/horseee/LLM-Pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from transformers import LlamaTokenizer\n",
    "from LLMPruner.models.hf_llama.modeling_llama import LlamaForCausalLM, LlamaRMSNorm\n",
    "\n",
    "import LLMPruner.torch_pruning as tp \n",
    "from LLMPruner.pruner import hf_llama_pruner as llama_pruner\n",
    "from LLMPruner.datasets.example_samples import get_examples\n",
    "from LLMPruner.templates.prompts import prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"Enoch/llama-7b-hf\"\n",
    "cache_dir = \"./llm_weights\"\n",
    "num_examples = 10\n",
    "iterative_steps = 1 #迭代次数\n",
    "taylor = 'param_first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_version = float('.'.join(torch.__version__.split('.')[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# 设置随机种子，方便复现结果\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc908d019934f0ebd934dece9e704ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    cache_dir=cache_dir,\n",
    "    low_cpu_mem_usage=True if torch_version >=1.9 else False\n",
    ")\n",
    "if device != \"cpu\":\n",
    "    model.half()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型效果\n",
    "def eval():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                max_length=128,\n",
    "                top_p=0.95,\n",
    "                temperature=1,\n",
    "            )\n",
    "            \n",
    "            result = tokenizer.decode(generation_output[0])\n",
    "            print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: <s> I believe the meaning of life is to live everyday as though it is your last and never take anyone for granted. Everyone you meet is fighting their own battle and it's important to treat others with the compassion they need.\n",
      "A woman must be twice as good as a man to go half as far, and that’s being conservative. That’s why woman have always had to work harder. ~ Elizabeth Cady Stanton</s>\n",
      "result: <s>Simply put, the theory of relativity states that 1) the speed of light is a constant for all observers, and that 2) the laws of physics behave differently depending upon the moving speed of the observer. Einstein didn't invent either of those notions. The first had been observed in 1676 when Ole Rømer determined that the speed of light is constant from all moving objects on Earth. The second had been proposed in 1887 by Hendrik Lorentz and independently by Georges Lemaître (the former first). Rømer, Lorentz and L\n",
      "result: <s>Building a website can be done in 10 simple steps:\n",
      "1. Set Your Goal\n",
      "Website development comes down to three basic building blocks: the purpose, the content and the technology. What is the purpose of the website? How does the site content support that purpose? And how does that purpose and content drive the technology we choose to build your website? These elements form the structure, or framework, of the website. When we get started, we have a simple discussion to determine your website goals, and then the project plan can get started.\n",
      "2. Determine Your Business Goals\n",
      "We will ask\n",
      "result: <s>Tweet: \"I hate it when my phone battery dies.\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"My day has been 👍\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This is the link to the article\"\n",
      "Sentiment: Neutral\n",
      "###\n",
      "Tweet: \"This new music video was incredibile\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"The new iPhone 7 is soooooo good!\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"\n",
      "result: <s>Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "\n",
      "peppermint => menthe poivrée\n",
      "\n",
      "plush girafe => girafe peluche\n",
      "\n",
      "cheese => fromage\n",
      "\n",
      "copper => cuivre\n",
      "\n",
      "### English/French translation\n",
      "\n",
      "See also: Category:Translations by Wiktionary\n",
      "\n",
      "Main articles: French-English translations, French/English dictionary, and English/French dictionary\n",
      "\n",
      "There have been many dictionaries and reference books that attempt to translate English-French, and French-\n"
     ]
    }
   ],
   "source": [
    "# 裁剪前测试模型效果\n",
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad_(True)\n",
    "    \n",
    "before_pruning_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "forward_prompts = torch.tensor([\n",
    "    [    1,   306,  4658,   278,  6593,   310,  2834,   338],\n",
    "    [    1,  3439, 17632,  1925, 29892,   278,  6368,   310],\n",
    "]).to(device) # Only for building the dependency graph. Any input will be fine since the computation result are not taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用泰勒方式计算重要性\n",
    "imp = llama_pruner.TaylorImportance(group_reduction=\"sum\", taylor=taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义需要的参数\n",
    "kwargs = {\n",
    "        \"importance\": imp,\n",
    "        \"global_pruning\": True,\n",
    "        \"iterative_steps\": 1, #迭代次数 \n",
    "        \"ch_sparsity\": 0.2, #稀疏率\n",
    "        \"ignored_layers\":[],\n",
    "        \"channel_groups\": {\n",
    "        },\n",
    "        \"consecutive_groups\": {\n",
    "        layer.self_attn.q_proj: layer.self_attn.head_dim for layer in model.model.layers\n",
    "        },\n",
    "        \"customized_pruners\": {\n",
    "        LlamaRMSNorm: llama_pruner.hf_rmsnorm_pruner,\n",
    "        },\n",
    "        \"root_module_types\": None, \n",
    "        \"root_instances\": [model.model.layers[i].self_attn.q_proj for i in range(3, 31)] +\n",
    "                        [model.model.layers[i].mlp.gate_proj for i in range(3, 31)] #裁剪3-31层\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Attention Layer = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Pruning MLP Layer = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Start Pruning\n",
      "Start Backwarding in iterative steps = 0...\n",
      "Loss = 2.48046875\n",
      "torch.Size([309120]) 56\n",
      "61824 0.2 309120\n",
      "After Iter 1/1, #parameters: 5978476544\n"
     ]
    }
   ],
   "source": [
    "print(\"Pruning Attention Layer = {}\".format(list(range(3, 31))))\n",
    "print(\"Pruning MLP Layer = {}\".format(list(range(3,31))))\n",
    "\n",
    "pruner = tp.pruner.MetaPruner(\n",
    "        model,\n",
    "        forward_prompts,\n",
    "        **kwargs\n",
    ")\n",
    "model.zero_grad()\n",
    "\n",
    "print(\"Start Pruning\")\n",
    "for i in range(iterative_steps):\n",
    "        example_prompts = get_examples('c4', tokenizer, num_examples, seq_len = 64).to(device)\n",
    "        print(\"Start Backwarding in iterative steps = {}...\".format(i))\n",
    "                \n",
    "        loss = model(example_prompts, labels=example_prompts).loss\n",
    "        print(\"Loss = {}\".format(loss))\n",
    "        loss.backward()\n",
    "\n",
    "        pruner.step()\n",
    "\n",
    "        after_pruning_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(\"After Iter {}/{}, #parameters: {}\".format(i+1, iterative_steps, after_pruning_parameters))\n",
    "\n",
    "        # modify inferece-related attributes\n",
    "        for layer in model.model.layers:\n",
    "                layer.self_attn.num_heads = layer.self_attn.q_proj.weight.data.shape[0] // layer.self_attn.head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the gradient in the model\n",
    "model.zero_grad()\n",
    "for name, module in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "                module.grad = None\n",
    "del pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Param before: 6738415616, #Param after: 5978476544, Ratio = 88.7223%\n"
     ]
    }
   ],
   "source": [
    "print(\"#Param before: {}, #Param after: {}, Ratio = {:.4f}%\".format(before_pruning_parameters, after_pruning_parameters,  100.0*after_pruning_parameters/before_pruning_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Requirement: 26024.59228515625 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((\"Memory Requirement: {} MiB\\n\".format(torch.cuda.memory_allocated()/1024/1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存裁剪后的模型\n",
    "torch.save({\n",
    "            'model': model, \n",
    "            'tokenizer': tokenizer,\n",
    "        }, 'model_llm_pruner.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half()\n",
    "model.to(device)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0 \n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: <s>I believe the meaning of life is to find a job that is of real value, that involves real skills and real craftsmanship and real human interactions with real other people. The whole point of life is that in a sense the whole point of life is the whole point of life. Life does not have a point except life; life is life, the point is life; life itself, life is what life is.\n",
      "Life is the art of taking, the process of keeping, and the use of an abundance. Life is living. Life is living. Life is life. Life is life. Life is life. Life\n",
      "result: <s>Simply put, the theory of relativity states that 2 physical objects in motion relative to each other experience a difference in time and speed due to the relative speed of the 2 objects relative to each other. The difference is called relativistic time dilation.\n",
      "The following video uses a clock with a ticking mechanism as a demonstration.\n",
      "The video shows how 2 clocks are accelerating relative to each other and shows how their time rates differ. For example, clock B shows a delay relative to clock A and clock A shows a delay relative clock clock B.\n",
      "In the video, the clocks are acceler\n",
      "result: <s>Building a website can be done in 10 simple steps:\n",
      "1. Create Your Website Layout\n",
      "2. Define Your Functional Requirements\n",
      "4. Prepare a Style Guide\n",
      "5. Write the Content\n",
      "7. Test Your Website\n",
      "9. Make Post-Launch Changes\n",
      "10. Make Promotional Materials\n",
      "Website Layout: This is the starting point of your website development. It defines the structure, navigation, and appearance of your website. This determines how information is presented, navigated, and found, as well as a lot of other features that are common to every website.\n",
      "result: <s>Tweet: \"I hate it when my phone battery dies.\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"My day has been 👍\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This is the link to the article\"\n",
      "Sentiment: Neutral\n",
      "###\n",
      "Tweet: \"This new music video was incredibile\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This new music video is incredible\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I watched this YouTube\n",
      "result: <s>Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "\n",
      "peppermint => menthe poivrée\n",
      "\n",
      "plush girafe => girafe peluche\n",
      "\n",
      "cheese => fromage\n",
      "\n",
      "English to French translation\n",
      "\n",
      "Input a English/English word, e.g. sea otter, and then the French version, loutre de mer. This is the reverse of the translator.\n",
      "\n",
      "Hint: French and English dictionaries work well with word-based inputs, and words of Latin and Greek origin are easier to find in English dictionaries\n"
     ]
    }
   ],
   "source": [
    "# 裁剪后测试模型效果\n",
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    max_new_tokens=128,\n",
    "    stream_output=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_length=max_new_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://38fc60b05fb5e39c23.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://38fc60b05fb5e39c23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用可视化界面测试，网址为ip:7860\n",
    "gr.Interface(\n",
    "    fn=evaluate,\n",
    "    inputs=[\n",
    "        gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=1, label=\"Temperature\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=0.95, label=\"Top p\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=100, step=1, value=50, label=\"Top k\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=1, maximum=2000, step=1, value=128, label=\"Max tokens\"\n",
    "        ),\n",
    "        gr.components.Checkbox(label=\"Stream output\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(\n",
    "            lines=5,\n",
    "            label=\"Output\",\n",
    "        )\n",
    "    ],\n",
    "    title=\"Evaluate Pruned Model\",\n",
    "    description=\"Pruned Model\",\n",
    ").queue().launch(server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pruner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
