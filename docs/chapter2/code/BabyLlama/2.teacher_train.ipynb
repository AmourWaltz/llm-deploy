{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ëøô‰ªΩ‰ª£Á†Å‰øÆÊîπËá™‰ªìÂ∫ìÔºö https://github.com/timinar/BabyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ËÆ≠ÁªÉÊïôÂ∏àÊ®°ÂûãGPT-2 Âíå Llama\n",
    "\n",
    "> ËÆ∫Êñá‰∏≠ÂÜôÂà∞Ôºö\"The GPT-2 model has 24 layers, 16 attention heads, an embedding dimension of 1536, intermediate size of 6144, and maximum sequence length of 128, resulting in 705M parameters. It was trained for 6 epochs with a batch size of 256 and maximum learning rate3 of 2.5 ¬∑ 10‚àí4. The LLaMA model has 24 layers, 8 attention heads, a hidden size of 1024, intermediate size of 3072, and maximum sequence length of 256, resulting in 360M parameters. It was trained for 4 epochs with a batch size of 128 and maximum learning rate of 3 ¬∑ 10‚àí4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1282184 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_10M_clean\\bnc_spoken.train len: 1282184\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_10M_clean\\childes.train len: 6301883\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_10M_clean\\gutenberg.train len: 3482661\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_10M_clean\\open_subtitles.train len: 3394685\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_10M_clean\\simple_wiki.train len: 2202434\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_10M_clean\\switchboard.train len: 249559\n",
      "Saving data to F:\\llm-deploy-data\\data\\Babyllama\\babylm_10M_clean\\tokenized_GPT2TokenizerFast_16000.pt\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_dev_clean\\bnc_spoken.dev len: 1749792\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_dev_clean\\childes.dev len: 5927646\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_dev_clean\\gutenberg.dev len: 3896232\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_dev_clean\\open_subtitles.dev len: 3466050\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_dev_clean\\simple_wiki.dev len: 2138217\n",
      "üî• F:\\llm-deploy-data\\data\\Babyllama\\babylm_dev_clean\\switchboard.dev len: 251354\n",
      "Saving data to F:\\llm-deploy-data\\data\\Babyllama\\babylm_dev_clean\\tokenized_GPT2TokenizerFast_16000.pt\n"
     ]
    }
   ],
   "source": [
    "# ÂáÜÂ§áÊï∞ÊçÆ\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2TokenizerFast\n",
    "from babylm_dataset import BabylmDataset\n",
    "from random import sample, seed\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "data_train_path = \"F:/llm-deploy-data/data/Babyllama/babylm_10M_clean\"\n",
    "data_eval_path = \"F:/llm-deploy-data/data/Babyllama/babylm_dev_clean\"\n",
    "tokenizer_path = \"F:/llm-deploy-data/data/Babyllama/models/gpt-clean-16000.json\"\n",
    "\n",
    "SEQ_LENGTH = 128\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file= str(tokenizer_path))\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.model_max_length = SEQ_LENGTH\n",
    "\n",
    "train_dataset = BabylmDataset(data_train_path, SEQ_LENGTH, tokenizer=tokenizer, random_chunk=True)\n",
    "full_eval_dataset = BabylmDataset(data_eval_path, SEQ_LENGTH, tokenizer=tokenizer, offset=0)\n",
    "\n",
    "seed(2024) # we fix the same subset for all models\n",
    "eval_indices = sample(range(len(full_eval_dataset)), 200)\n",
    "eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\dl2\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c592ad2c124e44d8abf7d6b77ab20639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ËÆ≠ÁªÉGPT2Ê®°Âûã\n",
    "from transformers import (\n",
    "    GPT2Config, GPT2LMHeadModel, \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = GPT2Config(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        n_positions=2*tokenizer.model_max_length,\n",
    "        n_embd=1536,\n",
    "        n_layer=24,\n",
    "        n_head=16,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "output_dir = \"./gpt2-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=256,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(2.5e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ËÆ≠ÁªÉLlamaÊ®°Âûã\n",
    "from transformers import (\n",
    "    LlamaConfig, LlamaForCausalLM,  \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = LlamaConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_position_embeddings=2*tokenizer.model_max_length,\n",
    "        hidden_size=1024,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=24,\n",
    "        num_attention_heads=8,\n",
    "        tie_word_embeddings=False,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = LlamaForCausalLM(model_config)\n",
    "\n",
    "output_dir = \"./gpt2-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=128,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(3e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
