{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这份代码修改自仓库： https://github.com/timinar/BabyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练教师模型GPT-2 和 Llama\n",
    "\n",
    "> 论文中写到：\n",
    ">\n",
    "> \"The GPT-2 model has 24 layers, 16 attention heads, an embedding dimension of 1536, intermediate size of 6144, and maximum sequence length of 128, resulting in 705M parameters. It was trained for 6 epochs with a batch size of 256 and maximum learning rate3 of 2.5 · 10−4. The LLaMA model has 24 layers, 8 attention heads, a hidden size of 1024, intermediate size of 3072, and maximum sequence length of 256, resulting in 360M parameters. It was trained for 4 epochs with a batch size of 128 and maximum learning rate of 3 · 10−4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/train_10M_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "🔥 数据集总大小: 16912909\n",
      "🔥 为了缩短训练时间，这里缩减为: 422822\n",
      "Loading data from data/dev_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "🔥 数据集总大小: 17428872\n",
      "🔥 为了缩短训练时间，这里缩减为: 435721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/workdir/llm-deploy/docs/chapter2/code/BabyLlama/babylm_dataset.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(tokenized_file)\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2TokenizerFast\n",
    "from babylm_dataset import BabylmDataset\n",
    "from random import sample, seed\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "data_train_path = \"./data/train_10M_clean\"\n",
    "data_eval_path = \"./data/dev_clean\"\n",
    "tokenizer_path = \"./models/gpt-clean-16000.json\"\n",
    "\n",
    "SEQ_LENGTH = 128\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file= str(tokenizer_path))\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.model_max_length = SEQ_LENGTH\n",
    "\n",
    "# 进入BsbylmDataset类，可以在初始化函数中修改数据集大小\n",
    "train_dataset = BabylmDataset(data_train_path, SEQ_LENGTH, tokenizer=tokenizer, random_chunk=True)\n",
    "full_eval_dataset = BabylmDataset(data_eval_path, SEQ_LENGTH, tokenizer=tokenizer, offset=0)\n",
    "\n",
    "seed(2024) # we fix the same subset for all models\n",
    "eval_indices = sample(range(len(full_eval_dataset)), 200)\n",
    "eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31ea8b21a604828bf402e0d6aee4b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练GPT2模型\n",
    "from transformers import (\n",
    "    GPT2Config, GPT2LMHeadModel, \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = GPT2Config(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        n_positions=2*tokenizer.model_max_length,\n",
    "        n_embd=1536,\n",
    "        n_layer=24,\n",
    "        n_head=16,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "output_dir = \"./models/gpt2-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(2.5e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False,\n",
    "    no_cuda = True,   # we use CPU，显卡足够大的话可以改为False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220bcd3b441f40609e1993ca00e9c9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.5414, 'grad_norm': 7.050642013549805, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.24}\n",
      "{'loss': 10.4487, 'grad_norm': 4.373862266540527, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.48}\n",
      "{'loss': 7.7475, 'grad_norm': 2.7600486278533936, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.72}\n",
      "{'loss': 6.8205, 'grad_norm': 2.7919507026672363, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f30fb59390345fa83872f5663072f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.165042877197266, 'eval_runtime': 48.2064, 'eval_samples_per_second': 4.149, 'eval_steps_per_second': 0.519, 'epoch': 1.0}\n",
      "{'loss': 6.1967, 'grad_norm': 2.381103515625, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.2}\n",
      "{'loss': 5.7663, 'grad_norm': 2.0445632934570312, 'learning_rate': 0.00011999999999999999, 'epoch': 1.45}\n",
      "{'loss': 5.6016, 'grad_norm': 1.9865704774856567, 'learning_rate': 0.00014, 'epoch': 1.69}\n",
      "{'loss': 5.4055, 'grad_norm': 2.2117745876312256, 'learning_rate': 0.00015999999999999999, 'epoch': 1.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd176b315e394edbb280861352f6e768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.207029342651367, 'eval_runtime': 43.6859, 'eval_samples_per_second': 4.578, 'eval_steps_per_second': 0.572, 'epoch': 2.0}\n",
      "{'loss': 5.113, 'grad_norm': 2.0187833309173584, 'learning_rate': 0.00017999999999999998, 'epoch': 2.17}\n",
      "{'loss': 4.9987, 'grad_norm': 1.7442786693572998, 'learning_rate': 0.00019999999999999998, 'epoch': 2.41}\n",
      "{'loss': 4.8864, 'grad_norm': 1.9827890396118164, 'learning_rate': 0.00021999999999999995, 'epoch': 2.65}\n",
      "{'loss': 4.7585, 'grad_norm': 1.594044804573059, 'learning_rate': 0.00023999999999999998, 'epoch': 2.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70818e0d10a4591ab1d61de22885413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.277788162231445, 'eval_runtime': 47.1181, 'eval_samples_per_second': 4.245, 'eval_steps_per_second': 0.531, 'epoch': 3.0}\n",
      "{'loss': 4.554, 'grad_norm': 1.9135853052139282, 'learning_rate': 0.00026, 'epoch': 3.13}\n",
      "{'loss': 4.4329, 'grad_norm': 1.7531003952026367, 'learning_rate': 0.00028, 'epoch': 3.37}\n",
      "{'loss': 4.4757, 'grad_norm': 1.717421293258667, 'learning_rate': 0.0003, 'epoch': 3.61}\n",
      "{'loss': 4.2759, 'grad_norm': 1.5224635601043701, 'learning_rate': 9.259748514523653e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77066ee69048466fb456e8cd49161a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.135180473327637, 'eval_runtime': 44.22, 'eval_samples_per_second': 4.523, 'eval_steps_per_second': 0.565, 'epoch': 4.0}\n",
      "{'train_runtime': 7478.7308, 'train_samples_per_second': 1.413, 'train_steps_per_second': 0.044, 'train_loss': 6.295483612152467, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-teacher/tokenizer_config.json',\n",
       " './gpt2-teacher/special_tokens_map.json',\n",
       " './gpt2-teacher/vocab.json',\n",
       " './gpt2-teacher/merges.txt',\n",
       " './gpt2-teacher/added_tokens.json',\n",
       " './gpt2-teacher/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练Llama模型\n",
    "from transformers import (\n",
    "    LlamaConfig, LlamaForCausalLM,  \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = LlamaConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_position_embeddings=2*tokenizer.model_max_length,\n",
    "        hidden_size=1024,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=24,\n",
    "        num_attention_heads=8,\n",
    "        tie_word_embeddings=False,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = LlamaForCausalLM(model_config)\n",
    "\n",
    "output_dir = \"./models/llama-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(3e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False,\n",
    "    no_cuda=True   # we use CPU，显卡足够大的话可以改为False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
