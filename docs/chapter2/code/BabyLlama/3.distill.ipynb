{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 蒸馏学生模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import  Subset\n",
    "from random import sample\n",
    "from pathlib import Path\n",
    "from babylm_dataset import BabylmDataset\n",
    "\n",
    "# 定义超参数\n",
    "#############\n",
    "LR = 2.5e-4\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 128\n",
    "\n",
    "TEMPERATURE = 2.0\n",
    "ALPHA = 0.5\n",
    "#############\n",
    "\n",
    "teacher_dir1 = './models/Llama-360M'\n",
    "teacher_dir2 = './models/gpt-705M'\n",
    "\n",
    "\n",
    "MODEL_NAME = f'Baby-Llama-58M'\n",
    "MODEL_OUTPUT = Path('./models') /  MODEL_NAME\n",
    "EVAL_SAMPLES = 1000\n",
    "\n",
    "tokenizer_path = \"F:/llm-deploy-data/data/Babyllama/models/gpt-clean-16000.json\"\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file= str(tokenizer_path))\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "\n",
    "# in the original code I had random_chunk = False\n",
    "# random_chunk=True is expected to improve the model performance a bit\n",
    "data_train_path = \"F:/llm-deploy-data/data/Babyllama/babylm_10M_clean\"\n",
    "data_eval_path = \"F:/llm-deploy-data/data/Babyllama/babylm_dev_clean\"\n",
    "train_dataset = BabylmDataset(data_train_path, SEQ_LENGTH, tokenizer=tokenizer, random_chunk=True)\n",
    "full_eval_dataset = BabylmDataset(data_eval_path, SEQ_LENGTH, tokenizer=tokenizer, offset=0)\n",
    "\n",
    "eval_indices = sample(range(len(full_eval_dataset)), EVAL_SAMPLES)\n",
    "eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = SEQ_LENGTH\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=16,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=8,\n",
    "    bos_token_id=tokenizer.convert_tokens_to_ids(\"<s>\"),\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(\"</s>\"),\n",
    "    pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    max_position_embeddings=2*SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "student = LlamaForCausalLM(config)\n",
    "# student = LlamaForCausalLM.from_pretrained(student_dir)\n",
    "\n",
    "\n",
    "teacher1 = LlamaForCausalLM.from_pretrained(teacher_dir1)\n",
    "teacher2 = GPT2LMHeadModel.from_pretrained(teacher_dir2)\n",
    "teachers = [teacher1, teacher2]\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(f'model num parameters: student = {student.num_parameters()}')\n",
    "print(f'model num parameters: teacher1 = {teacher1.num_parameters()}')\n",
    "print(f'model num parameters: teacher2 = {teacher2.num_parameters()}')\n",
    "\n",
    "\n",
    "\n",
    "#  Distillation Trainer\n",
    "#  We modified the Trainer from this repo https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker\n",
    "# to work with an ensemble of teachers\n",
    "\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_models=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teachers = teacher_models\n",
    "        for teacher in self.teachers:\n",
    "            # place each teacher on same device as student\n",
    "            self._move_model_to_device(teacher, self.model.device)\n",
    "            teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss = outputs_student.loss\n",
    "\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            all_teacher_logits = []\n",
    "            for teacher in self.teachers:\n",
    "                outputs_teacher = teacher(**inputs)\n",
    "                all_teacher_logits.append(outputs_teacher.logits)\n",
    "            avg_teacher_logits = torch.stack(all_teacher_logits).mean(dim=0)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == avg_teacher_logits.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (\n",
    "            loss_function(\n",
    "                F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "                F.softmax(avg_teacher_logits / self.args.temperature, dim=-1),\n",
    "            )\n",
    "            * (self.args.temperature ** 2)\n",
    "        )\n",
    "        # Return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1.0 - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    report_to=\"wandb\",\n",
    "    warmup_steps=200, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=LR,\n",
    "    logging_steps=20,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    weight_decay=0.1,\n",
    "    alpha=ALPHA,\n",
    "    temperature=TEMPERATURE,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "        student,\n",
    "        training_args,\n",
    "        teacher_models=teachers,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.save_model(MODEL_OUTPUT)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
