{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码参考了 https://github.com/facebookresearch/MetaICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import random\n",
    "import transformers\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152.95s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/workdir/llm-deploy/MetaICL\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'task': 'financial_phrasebank',\n",
       "   'input': 'In Finland , the city of Forssa has said it will not pay compensation to food industry companies HK Ruokatalo and Atria for the lye leak into tap water that occurred in March 2008 .',\n",
       "   'output': 'negative',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': \"In the reporting period , the company 's operating profit grew by 43.2 % to EUR 6 million .\",\n",
       "   'output': 'positive',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'Circulation revenue has increased by 5 % in Finland and 4 % in Sweden in 2008 .',\n",
       "   'output': 'positive',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'The business had gross written premiums of EUR152 .4 m (  91.5 m ) in 2000 , a net combined ratio of 133 % and 175 staff in total with offices in the UK , Germany and Benelux .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'The order also covers design services , hardware , software licences , as well as maintenance services over six years .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'Le Lay succeeds Walter G++nter and will be based in Finland .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'Bovine slaughtering and cutting at the Kuopio facility will be transferred to the Kauhajoki slaughterhouse .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'A Helsinki : ELIiV today reported EPS of EUR1 .13 for 2009 , an increase over EPS of EUR1 .12 in 2008 .',\n",
       "   'output': 'positive',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'Sarantel , based in Wellingborough , UK , designs high-performance antennas for portable wireless devices .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': \"The company 's board of directors would propose a dividend of EUR1 .00 per share for 2005 .\",\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': \"Glaston 's own glass processing unit , Tamglass Glass Processing , is a manufacturer of high quality safety glass products , and operates in Finland .\",\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': \"Capacity of the facility made by Finland 's Vaahto Group is 86,000 tons of light coated paper .\",\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'Panostaja did not disclose the purchase price .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'The total investment of the project will be approximately EUR 36m .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'Profit after taxes for the period was up to EUR0 .9 m , from EUR0 .01 m last year .',\n",
       "   'output': 'positive',\n",
       "   'options': ['negative', 'neutral', 'positive']},\n",
       "  {'task': 'financial_phrasebank',\n",
       "   'input': 'The issue came up in connection with discussion with local municipalities concerning the sale of water to industrial facilities .',\n",
       "   'output': 'neutral',\n",
       "   'options': ['negative', 'neutral', 'positive']}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "train_data_files = [\"data/financial_phrasebank/financial_phrasebank_16_100_train.jsonl\"]\n",
    "train_data = []\n",
    "for train_data_file in train_data_files:\n",
    "    with open(\"data/financial_phrasebank/financial_phrasebank_16_100_train.jsonl\", \"r\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    train_data.append(data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/imgfil/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish saving preprocessed data ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_length=1024\n",
    "max_length_per_example=256\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "def prepro_sentence_pair_single(ids1, ids2, max_length,\n",
    "                                bos_token_id, eos_token_id,\n",
    "                                allow_truncation=False):\n",
    "\n",
    "    #if bos_token_id is not None:\n",
    "    #    ids1 = [bos_token_id] + ids1\n",
    "    #if eos_token_id is not None:\n",
    "    #    ids2 = ids2 + [eos_token_id]\n",
    "    if allow_truncation and len(ids1)+len(ids2) > max_length:\n",
    "        ids1 = ids1[len(ids1)+len(ids2)-max_length:] # len = max_length-len(ids2)\n",
    "        assert len(ids1)+len(ids2)==max_length\n",
    "\n",
    "    n_mask = max_length-len(ids1)-len(ids2)\n",
    "    assert n_mask>=0, (max_length, len(ids1), len(ids2))\n",
    "    input_ids = ids1+ids2+[0 for _ in range(n_mask)]\n",
    "    attention_mask = [1 for _ in ids1+ids2] + [0 for _ in range(n_mask)]\n",
    "    token_type_ids = [0 for _ in ids1] + [1 for _ in ids2] + [0 for _ in range(n_mask)]\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "\n",
    "def _prepro_each_datapoint(dp, is_first=True, is_training=False, for_demonstrations=False):\n",
    "    dp = dp.copy()\n",
    "\n",
    "    no_label = np.all([option==\"\" for option in dp[\"options\"]])\n",
    "    no_input = dp[\"input\"]==\"\"\n",
    "    if not is_first:\n",
    "        dp[\"output\"] = \"\\n\\n\\n\" + dp[\"output\"]\n",
    "        if \"options\" in dp:\n",
    "            dp[\"options\"] = [\"\\n\\n\\n\" + opt for opt in dp[\"options\"]]\n",
    "    if not no_input:\n",
    "        if not no_label:\n",
    "            dp[\"input\"] = \"\\n\" + dp[\"input\"]\n",
    "\n",
    "    input_tokens = tokenizer(dp[\"input\"])[\"input_ids\"]\n",
    "\n",
    "    if is_training or for_demonstrations:\n",
    "        output_tokens = tokenizer(dp[\"output\"])[\"input_ids\"]\n",
    "\n",
    "        if \"task\" in dp:\n",
    "            if len(input_tokens)>=max_length_per_example - 2 - len(output_tokens):\n",
    "                if dp[\"task\"].startswith(\"inst:\") and len(input_tokens)<len(output_tokens):\n",
    "                    output_tokens = output_tokens[:max_length_per_example - 2 - len(input_tokens)]\n",
    "                else:\n",
    "                    input_tokens = input_tokens[:max_length_per_example - 2 - len(output_tokens)]\n",
    "\n",
    "        assert len(input_tokens)+len(output_tokens)+2<=max_length_per_example, \\\n",
    "            (dp.get(\"task\", None), len(input_tokens), len(output_tokens), max_length_per_example)\n",
    "\n",
    "        return output_tokens, input_tokens\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert len(dp[\"options\"])>=2, dp\n",
    "        assert dp[\"output\"] in dp[\"options\"]\n",
    "        option_tokens = [tokenizer(option)[\"input_ids\"] for option in dp[\"options\"]]\n",
    "        option_length = np.max([len(option) for option in option_tokens])\n",
    "\n",
    "        if len(input_tokens)>=max_length_per_example - 2 - option_length:\n",
    "            input_tokens = input_tokens[:max_length_per_example - 2 - option_length]\n",
    "\n",
    "        input_tokens = [input_tokens for _ in option_tokens]\n",
    "        output_tokens = option_tokens\n",
    "        option_tokens = [dp[\"options\"].index(dp[\"output\"])]\n",
    "\n",
    "        return output_tokens, input_tokens, option_tokens\n",
    "\n",
    "\n",
    "def _tensorize_for_training(train_data):\n",
    "    for dp in train_data:\n",
    "        assert type(dp)==dict, (\"Each example should be a dictionary\", dp)\n",
    "        assert \"input\" in dp and \"output\" in dp, (\"Training example should contain input and output\", dp)\n",
    "\n",
    "    # each datapoint: passage, question, options, output\n",
    "    bos_token_id = tokenizer.bos_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids = [], [], []\n",
    "\n",
    "\n",
    "    for dp in train_data:\n",
    "        inputs, outputs = _prepro_each_datapoint(\n",
    "            dp, is_first=True, is_training=True)\n",
    "\n",
    "        encoded = prepro_sentence_pair_single(\n",
    "            inputs, outputs, max_length, bos_token_id, eos_token_id)\n",
    "\n",
    "        input_ids.append(encoded[0])\n",
    "        attention_mask.append(encoded[1])\n",
    "        token_type_ids.append(encoded[2])\n",
    "\n",
    "    return dict(input_ids=torch.LongTensor(input_ids),\n",
    "                attention_mask=torch.LongTensor(attention_mask),\n",
    "                token_type_ids=torch.LongTensor(token_type_ids))\n",
    "\n",
    "# 数据转为tensor\n",
    "def tensorize_for_training(train_data):\n",
    "    inputs = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "    for in_ in train_data:\n",
    "        out = _tensorize_for_training(in_)\n",
    "        for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "            inputs[key] += out[key].numpy().tolist()\n",
    "\n",
    "    N = len(inputs[\"input_ids\"])\n",
    "    indices = np.random.permutation(range(N))\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = np.array(v)[indices]\n",
    "\n",
    "    with open(\"preprocessed_data\", \"wb\") as f:\n",
    "        pkl.dump({k:v for k, v in inputs.items()}, f)\n",
    "    print(\"Finish saving preprocessed data ...\")\n",
    "\n",
    "tensorize_for_training(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metaicl_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmetaicl_data\u001b[49m\u001b[38;5;241m.\u001b[39mprint_tensorized_example()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metaicl_data' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# 指定日志路径\n",
    "log_file = \"log.txt\"\n",
    "out_dir = \"output\"\n",
    "handlers = [logging.StreamHandler()]\n",
    "handlers.append(logging.FileHandler(log_file))\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=handlers)\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(out_dir)\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "model = MetaICLModel(logger, args.out_dir, args.fp16, args.local_rank)\n",
    "model.load(args.init_checkpoint, args.gpt2)\n",
    "model.to_device()\n",
    "model.setup_optimizer(args.optimization, num_training_steps, args.lr,\n",
    "                                args.weight_decay, args.warmup_steps)\n",
    "model.parallel()\n",
    "model.train()\n",
    "model.do_train(metaicl_data, args.batch_size, num_training_steps, save_period, log_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdeploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
