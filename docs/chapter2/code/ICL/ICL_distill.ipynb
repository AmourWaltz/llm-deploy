{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码参考了 https://github.com/facebookresearch/MetaICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import random\n",
    "import transformers\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152.95s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/workdir/llm-deploy/MetaICL\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/imgfil/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/imgfil/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024])\n",
      "[tensor([[31591,   198,   818,  ...,     0,     0,     0],\n",
      "        [31591,   198,   818,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "with open(\"data/financial_phrasebank/financial_phrasebank_16_100_train.jsonl\", \"r\") as f:\n",
    "    train_data = []\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "train_data[:3]\n",
    "\n",
    "max_length=1024\n",
    "max_length_per_example=256\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def prepro_sentence_pair_single(ids1, ids2, max_length,\n",
    "                                bos_token_id, eos_token_id,\n",
    "                                allow_truncation=False):\n",
    "\n",
    "    #if bos_token_id is not None:\n",
    "    #    ids1 = [bos_token_id] + ids1\n",
    "    #if eos_token_id is not None:\n",
    "    #    ids2 = ids2 + [eos_token_id]\n",
    "    if allow_truncation and len(ids1)+len(ids2) > max_length:\n",
    "        ids1 = ids1[len(ids1)+len(ids2)-max_length:] # len = max_length-len(ids2)\n",
    "        assert len(ids1)+len(ids2)==max_length\n",
    "\n",
    "    n_mask = max_length-len(ids1)-len(ids2)\n",
    "    assert n_mask>=0, (max_length, len(ids1), len(ids2))\n",
    "    input_ids = ids1+ids2+[0 for _ in range(n_mask)]\n",
    "    attention_mask = [1 for _ in ids1+ids2] + [0 for _ in range(n_mask)]\n",
    "    token_type_ids = [0 for _ in ids1] + [1 for _ in ids2] + [0 for _ in range(n_mask)]\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "def _prepro_each_datapoint(dp, is_first=True, is_training=False, for_demonstrations=False):\n",
    "    dp = dp.copy()\n",
    "\n",
    "    no_label = np.all([option==\"\" for option in dp[\"options\"]])\n",
    "    no_input = dp[\"input\"]==\"\"\n",
    "    if not is_first:\n",
    "        dp[\"output\"] = \"\\n\\n\\n\" + dp[\"output\"]\n",
    "        if \"options\" in dp:\n",
    "            dp[\"options\"] = [\"\\n\\n\\n\" + opt for opt in dp[\"options\"]]\n",
    "    if not no_input:\n",
    "        if not no_label:\n",
    "            dp[\"input\"] = \"\\n\" + dp[\"input\"]\n",
    "\n",
    "    input_tokens = tokenizer(dp[\"input\"])[\"input_ids\"]\n",
    "\n",
    "    if is_training or for_demonstrations:\n",
    "        output_tokens = tokenizer(dp[\"output\"])[\"input_ids\"]\n",
    "\n",
    "        if \"task\" in dp:\n",
    "            if (dp[\"task\"].startswith(\"inst:piqa\") or dp[\"task\"].startswith(\"inst:yahoo_answers_topics\")) and \\\n",
    "                    len(input_tokens)+len(output_tokens)+2>max_length_per_example:\n",
    "                input_tokens = input_tokens[:max_length_per_example // 2]\n",
    "                output_tokens = output_tokens[:max_length_per_example // 2 - 2]\n",
    "\n",
    "            elif len(input_tokens)>=max_length_per_example - 2 - len(output_tokens):\n",
    "                if dp[\"task\"].startswith(\"inst:\") and len(input_tokens)<len(output_tokens):\n",
    "                    output_tokens = output_tokens[:max_length_per_example - 2 - len(input_tokens)]\n",
    "                else:\n",
    "                    input_tokens = input_tokens[:max_length_per_example - 2 - len(output_tokens)]\n",
    "\n",
    "        assert len(input_tokens)+len(output_tokens)+2<=max_length_per_example, \\\n",
    "            (dp.get(\"task\", None), len(input_tokens), len(output_tokens), max_length_per_example)\n",
    "\n",
    "        return output_tokens, input_tokens\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert len(dp[\"options\"])>=2, dp\n",
    "        assert dp[\"output\"] in dp[\"options\"]\n",
    "        option_tokens = [tokenizer(option)[\"input_ids\"] for option in dp[\"options\"]]\n",
    "        option_length = np.max([len(option) for option in option_tokens])\n",
    "\n",
    "        if len(input_tokens)>=max_length_per_example - 2 - option_length:\n",
    "            input_tokens = input_tokens[:max_length_per_example - 2 - option_length]\n",
    "\n",
    "        input_tokens = [input_tokens for _ in option_tokens]\n",
    "        output_tokens = option_tokens\n",
    "        option_tokens = [dp[\"options\"].index(dp[\"output\"])]\n",
    "\n",
    "        return output_tokens, input_tokens, option_tokens\n",
    "\n",
    "\n",
    "def _tensorize_for_training(train_data):\n",
    "    for dp in train_data:\n",
    "        assert type(dp)==dict, (\"Each example should be a dictionary\", dp)\n",
    "        assert \"input\" in dp and \"output\" in dp, (\"Training example should contain input and output\", dp)\n",
    "\n",
    "    # each datapoint: passage, question, options, output\n",
    "    bos_token_id = tokenizer.bos_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids = [], [], []\n",
    "    n_answers = []\n",
    "\n",
    "\n",
    "    for dp in train_data:\n",
    "        inputs, outputs = _prepro_each_datapoint(\n",
    "            dp, is_first=True, is_training=True)\n",
    "\n",
    "        encoded = prepro_sentence_pair_single(\n",
    "            inputs, outputs, max_length, bos_token_id, eos_token_id)\n",
    "\n",
    "        input_ids.append(encoded[0])\n",
    "        attention_mask.append(encoded[1])\n",
    "        token_type_ids.append(encoded[2])\n",
    "\n",
    "    return dict(input_ids=torch.LongTensor(input_ids),\n",
    "                attention_mask=torch.LongTensor(attention_mask),\n",
    "                token_type_ids=torch.LongTensor(token_type_ids))\n",
    "\n",
    "# 数据转为tensor\n",
    "def tensorize_for_training(train_data, keyword=\"SST-2\", seed=0):\n",
    "\n",
    "    # tensorize_dir = \"tensorized\"\n",
    "    # if not os.path.exists(tensorize_dir):\n",
    "    #     os.makedirs(tensorize_dir)\n",
    "\n",
    "    # method = \"channel\"\n",
    "    # method_name = method\n",
    "    # k_name = len(train_data)\n",
    "    # length_name = max_length\n",
    "    # postfix = \"\"\n",
    "\n",
    "    # tensorize_path = os.path.join(tensorize_dir,\n",
    "    #                                 \"{}_{}_k={}_seed={}_length={}{}-rank=%d.pkl\".format(\n",
    "    #                                     keyword, method_name, k_name, seed, length_name,\n",
    "    #                                     postfix))\n",
    "\n",
    "    # print(tensorize_path)\n",
    "    # all_tensorize_paths = [tensorize_path % i for i in range(n_gpu)]\n",
    "\n",
    "    # unique_task_names = set([dp[\"task\"] for dp in train_data])\n",
    "    sharded_inputs = train_data\n",
    "\n",
    "    inputs = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "    for in_ in sharded_inputs:\n",
    "        out = _tensorize_for_training(in_)\n",
    "        for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "            inputs[key] += out[key].numpy().tolist()\n",
    "\n",
    "    N = len(inputs[\"input_ids\"])\n",
    "    indices = np.random.permutation(range(N))\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = np.array(v)[indices]\n",
    "\n",
    "    with open(\"preprocessed_data\", \"wb\") as f:\n",
    "        pkl.dump({k:v for k, v in inputs.items()}, f)\n",
    "    print(\"Finish saving preprocessed data ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from metaicl.data import MetaICLData\n",
    "from metaicl.model import MetaICLModel\n",
    "\n",
    "data = MetaICLData(method=\"channel\", max_length=1024, max_length_per_example=256)\n",
    "input1 = \"Both operating profit and net sales for the six-month period increased as compared to the corresponding period in 2007.\"\n",
    "data.tensorize(train_data, [input1], options=[\"positive\", \"neutral\", \"negative\"])\n",
    "dataloader = data.get_dataloader(2, is_training=True)\n",
    "for batch in dataloader:\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/imgfil/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging from MetaICLModel:\t Setting up for local_rank=-1, world_size=1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "info() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m MetaICLData(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, max_length_per_example\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m MetaICLModel()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchannel-metaicl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/workdir/llm-deploy/MetaICL/metaicl/model.py:110\u001b[0m, in \u001b[0;36mMetaICLModel.load\u001b[0;34m(self, checkpoint, gpt2)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReusing checkpoint at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m checkpoint)\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m in \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m         download_file(_id, checkpoint)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(checkpoint), checkpoint\n",
      "\u001b[0;31mTypeError\u001b[0m: info() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "from metaicl.data import MetaICLData\n",
    "from metaicl.model import MetaICLModel\n",
    "\n",
    "# Load the model\n",
    "data = MetaICLData(method=\"channel\", max_length=1024, max_length_per_example=256)\n",
    "model = MetaICLModel()\n",
    "model.load(\"channel-metaicl\")\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Make a prediction for `input1`\n",
    "input1 = \"Both operating profit and net sales for the six-month period increased as compared to the corresponding period in 2007.\"\n",
    "data.tensorize(train_data, [input1], options=[\"positive\", \"neutral\", \"negative\"])\n",
    "prediction = model.do_predict(data)[0]\n",
    "print (prediction) # positive\n",
    "\n",
    "# Make another prediction for `input2`\n",
    "input2 = \"The deal will have no significant effect on the acquiring company's equity ratio.\"\n",
    "data.tensorize(train_data, [input2], options=[\"positive\", \"neutral\", \"negative\"])\n",
    "prediction = model.do_predict(data)[0]\n",
    "print (prediction) # neutral"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdeploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
