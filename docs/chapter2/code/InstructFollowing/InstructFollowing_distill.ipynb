{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.28.1\n",
    "%pip install torch==2.1.2\n",
    "%pip install datasets==2.14.5\n",
    "\n",
    "# 更多环境信息，可以查看 code/env.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要包\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据\n",
    "# https://github.com/gyfffffff/llm-deploy/releases/tag/IF_data\n",
    "# 将下载的两个json文件放入同级目录data下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载模型\n",
    "\n",
    "# windows\n",
    "%pip install -U \"huggingface-hub[cli]\"\n",
    "!$env:HF_ENDPOINT = \"https://hf-mirror.com\"\n",
    "!huggingface-cli download --resume-download openai-community/gpt2 --local-dir ../../models/GPT-2\n",
    "\n",
    "# linux\n",
    "%pip install -U \"huggingface-hub[cli]\"\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "!huggingface-cli download --resume-download openai-community/gpt2 --local-dir ../../models/GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m flatdata \u001b[38;5;241m=\u001b[39m get_flat_data(datapath)\n\u001b[1;32m     63\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_dict(flatdata)\n\u001b[0;32m---> 64\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 使用map方法对数据集进行批处理\u001b[39;00m\n\u001b[1;32m     65\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     66\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmdeploy/lib/python3.9/site-packages/datasets/arrow_dataset.py:1289\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1287\u001b[0m test_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1289\u001b[0m update_data \u001b[38;5;241m=\u001b[39m \u001b[43mdoes_function_return_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting finished, running the mapping function on the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmdeploy/lib/python3.9/site-packages/datasets/arrow_dataset.py:1276\u001b[0m, in \u001b[0;36mDataset.map.<locals>.does_function_return_dict\u001b[0;34m(inputs, indices)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     all_dict_values_are_lists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(value, allowed_batch_return_types) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   1274\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_dict_values_are_lists \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 1276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `function` which is applied to all elements of table returns a `dict` of types \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. When using `batched=True`, make sure provided `function` returns a `dict` of types like `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1278\u001b[0m                 [\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mvalues()], allowed_batch_return_types\n\u001b[1;32m   1279\u001b[0m             )\n\u001b[1;32m   1280\u001b[0m         )\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m does_return_dict\n",
      "\u001b[0;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`."
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "# datapath = 'data/alpaca_gpt4_data_dev.json'  # dev数据只有少量数据，用于开发，实际训练时请使用full数据集\n",
    "datapath = 'data/alpaca_gpt4_data.json'  # full数据集\n",
    "\n",
    "# 定义tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "model_path = \"../../models/GPT-2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 定义数据集类\n",
    "def get_flat_data(datapath):\n",
    "    data = json.load(open(datapath, 'r'))\n",
    "    inputs = []  # 指令和输入\n",
    "    labels = []  # 输出\n",
    "\n",
    "    for item in data:\n",
    "        inputs.append(f\"{item['instruction']} {item['input']}\")\n",
    "        labels.append(item['output']) \n",
    "    return {\n",
    "        'input': inputs,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "def tokenize(example):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    input_token = tokenizer(\n",
    "        example['input'],\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    iids = input_token['input_ids']\n",
    "    label_token = tokenizer(\n",
    "        example['label'],\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    lids = label_token['input_ids']\n",
    "\n",
    "    lengths = []\n",
    "    input_ids = []\n",
    "    label_ids = []\n",
    "    attention_mask = []\n",
    "    for iid, lid in zip(iids, lids):\n",
    "        lengths.append(len(iid) + len(lid))\n",
    "        input_ids.append(iid + lid)\n",
    "        label_ids.append([-100]*len(iid) + lid)\n",
    "        attention_mask.append([1]*(len(iid) + len(lid)))\n",
    "    \n",
    "    lengths = torch.tensor(lengths)\n",
    "    pad_length = (lengths.max() - lengths).tolist()\n",
    "    for i, l in enumerate(pad_length):\n",
    "        input_ids[i] = [tokenizer.pad_token_id]*l + input_ids[i]\n",
    "        attention_mask[i] = [0]*l + attention_mask[i]\n",
    "        label_ids[i] = [-100]*l + label_ids[i]\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(label_ids, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# 创建数据集\n",
    "flatdata = get_flat_data(datapath)\n",
    "dataset = datasets.Dataset.from_dict(flatdata)\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=len(dataset))  # 使用map方法对数据集进行批处理\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 8.52311897277832\n",
      "val step: 0, loss: 5.500304698944092\n",
      "epoch: 1, step: 0, loss: 5.841464042663574\n",
      "val step: 0, loss: 4.260898113250732\n",
      "epoch: 2, step: 0, loss: 4.249709129333496\n",
      "val step: 0, loss: 3.278724193572998\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "# 定义模型\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "def train_step(batch):\n",
    "    kwargs = {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "        \"labels\": batch[\"labels\"],\n",
    "    }\n",
    "    res = model(**kwargs)[\"loss\"]\n",
    "    return res\n",
    "\n",
    "def val():\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        loss = train_step(batch)\n",
    "        if step % 10 == 0:\n",
    "            print(f\"val step: {step}, loss: {loss.item()}\")\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    model.to('cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    for epoch in range(3):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            loss = train_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if step % 10 == 0:\n",
    "                print(f\"epoch: {epoch}, step: {step}, loss: {loss.item()}\")\n",
    "        torch.save(model.state_dict(), f\"model_{epoch}.pt\")\n",
    "        val()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". me know if thoughts on this matter.. time you think it's to be here.\n",
      "\n",
      "celona is Spain\"\n",
      "\n",
      ". me know if thoughts on this matter.. time you think it's to be here.\n",
      "\n",
      "celona\" Spain\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试效果\n",
    "def test(trained=True):\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    if trained:\n",
    "        gpt2.load_state_dict(torch.load('model_2.pt'))\n",
    "    gpt2.eval()\n",
    "    gpt2.to('cpu')\n",
    "    \n",
    "    text = \"Please let me know your thoughts on the given place and why you think it deserves to be visited. \\\"Barcelona, Spain\\\"\"\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = gpt2(**encoded_input)\n",
    "    logits = output.logits\n",
    "    predicted_index = torch.argmax(logits, dim=-1)\n",
    "    predicted_text = tokenizer.decode(predicted_index[0])\n",
    "    print(predicted_text)\n",
    "\n",
    "test(trained=True)\n",
    "test(trained=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
