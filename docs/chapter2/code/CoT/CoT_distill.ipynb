{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码参考了 https://github.com/itsnamgyu/reasoning-teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 在命令行安装必要包\n",
    "```\n",
    "pip install -r code/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要包\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 98\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m dataloader(val_data_path)\n\u001b[1;32m    100\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m dataloader(test_data_path)\n",
      "Cell \u001b[0;32mIn[2], line 94\u001b[0m, in \u001b[0;36mdataloader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     92\u001b[0m flat_data \u001b[38;5;241m=\u001b[39m get_flat_data(data)\n\u001b[1;32m     93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_dict(flat_data)\n\u001b[0;32m---> 94\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmdeploy/lib/python3.9/site-packages/datasets/arrow_dataset.py:1289\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1287\u001b[0m test_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1289\u001b[0m update_data \u001b[38;5;241m=\u001b[39m \u001b[43mdoes_function_return_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting finished, running the mapping function on the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmdeploy/lib/python3.9/site-packages/datasets/arrow_dataset.py:1276\u001b[0m, in \u001b[0;36mDataset.map.<locals>.does_function_return_dict\u001b[0;34m(inputs, indices)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     all_dict_values_are_lists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(value, allowed_batch_return_types) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   1274\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_dict_values_are_lists \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 1276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `function` which is applied to all elements of table returns a `dict` of types \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. When using `batched=True`, make sure provided `function` returns a `dict` of types like `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1278\u001b[0m                 [\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mvalues()], allowed_batch_return_types\n\u001b[1;32m   1279\u001b[0m             )\n\u001b[1;32m   1280\u001b[0m         )\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m does_return_dict\n",
      "\u001b[0;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`."
     ]
    }
   ],
   "source": [
    "# 定义接下来要用到的变量\n",
    "model_name_or_path = \"../../models/GPT-2\"  # 替换成自己的模型路径\n",
    "# model_type = \"decoder\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 定义数据路径和dataloader\n",
    "train_data_path = \"../../data/F_zs_cot_date_understanding_good_train.jsonl\"\n",
    "val_data_path = \"../../data/F_zs_cot_date_understanding_good_val.jsonl\"\n",
    "test_data_path = \"../../data/F_zs_cot_date_understanding_good_test.jsonl\"\n",
    "\n",
    "def load_finetune_data(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_flat_data(data):\n",
    "    input_format = \"{sample[question]} ###\"\n",
    "    label_format = \" {sample[reasoning_completion]} --> {sample[answer]}\"\n",
    "    # 也可以使用下面的格式\n",
    "    # input_format = \"Q: {sample[question]}\\n\\nA: Let's think step by step.\\n\\n\"\n",
    "    # label_format = \" {sample[reasoning_completion]}\\n\\nTherefore the answer is {sample[answer]}\"\n",
    "    flat_data = {\n",
    "        \"input\": [],\n",
    "        \"label\": [],\n",
    "    }\n",
    "\n",
    "    for key, samples in data[\"data\"].items():\n",
    "        for sample in samples:\n",
    "            flat_data[\"input\"].append(input_format.format(sample=sample))\n",
    "            flat_data[\"label\"].append(label_format.format(sample=sample))\n",
    "    return flat_data\n",
    "\n",
    "model_type = \"decoder\"\n",
    "\n",
    "    \n",
    "def tokenize(example):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if model_type == \"decoder\":\n",
    "        # Tokenize and apply left side padding manually\n",
    "\n",
    "        # Tokenize in vanilla Python list form\n",
    "        it = tokenizer(\n",
    "            example[\"input\"],\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        iids = it[\"input_ids\"]\n",
    "        if \"label\" in example:\n",
    "            lids = tokenizer(\n",
    "                example[\"label\"],\n",
    "                max_length=512,\n",
    "                truncation=True\n",
    "            )[\"input_ids\"]\n",
    "        else:\n",
    "            lids = [list() for _ in range(len(iids))]\n",
    "\n",
    "        lengths = []\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        label_ids = []\n",
    "        for iid, lid in zip(iids, lids):\n",
    "            lengths.append(len(iid) + len(lid))\n",
    "            input_ids.append(iid + lid)\n",
    "            attention_mask.append([1] * (len(iid) + len(lid)))\n",
    "            label_ids.append([-100] * len(iid) + lid)\n",
    "\n",
    "        # Pad full sequences\n",
    "        lengths = torch.tensor(lengths)\n",
    "        pad_lengths = (lengths.max() - lengths).tolist()\n",
    "        for i, l in enumerate(pad_lengths):\n",
    "            # Apply left side padding\n",
    "            # Why? https://github.com/huggingface/transformers/issues/3021#issuecomment-1231526631\n",
    "            input_ids[i] = [tokenizer.pad_token_id] * l + input_ids[i]\n",
    "            attention_mask[i] = [0] * l + attention_mask[i]\n",
    "            label_ids[i] = [-100] * l + label_ids[i]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "    else:\n",
    "        raise NotImplementedError(model_type)\n",
    "\n",
    "def dataloader(path):\n",
    "    data = load_finetune_data(path)\n",
    "    flat_data = get_flat_data(data)\n",
    "    dataset = datasets.Dataset.from_dict(flat_data)\n",
    "    dataset = dataset.map(tokenize, batched=True, batch_size=len(dataset))\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    return DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "train_dataloader = dataloader(train_data_path)\n",
    "val_dataloader = dataloader(val_data_path)\n",
    "test_dataloader = dataloader(test_data_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    kwargs = {\n",
    "        \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "        \"labels\": batch[\"labels\"].to(device),\n",
    "    }\n",
    "    res = model(**kwargs)[\"loss\"]\n",
    "    return res\n",
    "\n",
    "def val():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            loss = train_step(batch)\n",
    "            print(\"Val Step: {}, Loss: {}\".format(step, loss.item()))\n",
    "def train():\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    for i in range(20):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            loss = train_step(batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if step % 4 == 0:\n",
    "                print(\"Step: {}, Loss: {}\".format(step, loss.item()))\n",
    "                torch.save(model.state_dict(), \"model_step_{}.pt\".format(i))\n",
    "                val()\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(ckpt_path=\"\"):\n",
    "    if ckpt_path:\n",
    "        model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_dataloader):\n",
    "            kwargs = {\n",
    "                \"input_ids\": batch[\"input_ids\"],\n",
    "                \"attention_mask\": batch[\"attention_mask\"],\n",
    "                \"labels\": batch[\"labels\"],\n",
    "            }\n",
    "            res = model(**kwargs)\n",
    "            loss = res[\"loss\"]\n",
    "            print(\"Test Step: {}, Loss: {}\".format(step, loss.item()))\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
