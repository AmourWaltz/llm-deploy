# 白盒蒸馏

## 1. 什么是白盒蒸馏
对于开源的大模型，我们可以获得一切的模型推理时数据，包括token输出的概率分布。这种能够获得token输出概率分布的场景，可以被看作“白盒”场景。反之是黑盒场景。利用白盒所提供的数据进行蒸馏，是白盒蒸馏。

<!-- ## 何时使用白盒蒸馏
传统的蒸馏针对的场景是分类任务，或者是让一个小模型模仿黑盒LLM（如GPT4）的API。那么如何将白盒LLM的知识传授给小模型呢？这其实是有专门的方法的，也就是这一章要讲的白盒蒸馏。

如何模型在输出下一个token之前，会先输出一个token的概率分布，然后从中采样一个token作为输出token。如果能够获取token的概率分布，那么就可以做白盒蒸馏，否则只能黑盒蒸馏。 -->

接下来我们会介绍两种经典的大模型白盒蒸馏的方法，MiniLLM和GKD。


# 2. MiniLLM

大模型能力的强大也伴随着参数量的膨胀，为了以合理的成本部署大模型，如何将大模型的知识蒸馏到小模型是一个问题。从前，面对有限的状态空间（比如有限的分类类别），教师模型和学生模型的参数量都足以学习每一种类别的模式；而在大模型自回归生成的场景下，学生模型参数变少后，天然地失去了和大模型同等的表达能力，从而传统的蒸馏可能效果不佳。

<!-- 传统的知识蒸馏是面向分类等有限状态空间设计的，通过最小化前向KL散度，就能够让学生模型（小模型）学到有限的状态空闲（比如有限的类别）。 -->

<!-- 这种蒸馏方式仍然适合大语言模型吗？大语言模型本质上是做自回归生成任务，每一步都是一个状态空间巨大的分类任务，每下一步都会基于之前的分类结果。MiniLLM的论文中指出，传统蒸馏方式不再适用与大语言模型蒸馏。 -->

<!-- 但是大语言模型本质上做的是自回归式生成任务，传统的知识蒸馏方法不再适用。 -->

MiniLLM是一种针对生成式语言模型的全新的KD方法，它是一种白盒蒸馏方法，这种方法使用逆向KL散度，理论上使得学生模型模仿教师模型概率较大的生成结果，忽略教师模型概率不大的生成结果。这样做一定程度放弃了模型生成的多样性，从而实现高性价比的LLM部署落地。

## 2.1 前向KL散度
前向KL散度是传统蒸馏时使用的损失函数，这里我们再复习一下它的概念：

假设老师分布为$p$, 学生分布为$q_\theta$, $\theta$ 是学生模型的参数。

前向KL散度可以看成是两个分布相似程度的定义（注意KL散度具有不对称性，不是距离）：

$KL(p||q_\theta) = \sum_i p(i)log\frac{p(i)}{q_\theta(i)}$。

<!-- 一般都要最小化KL散度。 -->

从定义可以看出，在$p$分布为$0$的地方，$q$分布无论为多少，都不影响这一项为$0$，所以当我们最小化前向KL散度时，$q$会在老师概率分布小的地方分配大的概率。对应到大模型生成上，就是在老师模型输出可能性很小的地方，学生模型却放大了这种可能性，显然这是不符合模型生成预期的。

## 2.2 逆向KL散度

reversed KL:

$$KL(q_\theta ||p) = \sum_i q_\theta(i)log\frac{q_\theta(i)}{p(i)} = -\sum_i q_\theta(i)log\frac{p(i)}{q_\theta(i)}$$

蒸馏时，使用逆向KL散度代替前向KL散度。最小化逆向KL散度时，老师分布大的地方，学生分布也同步变大，而老师分布小的地方，学生分布会更小。下面这张经典的图片可以看出前向和后向KL的差异。

![images\v2-543575cc0a0efdaccbd1d24570b8e9e4_b.png](images\v2-543575cc0a0efdaccbd1d24570b8e9e4_b.png)

这样直观上看，使用逆向KL散度更加符合生成模型的场景。



## 2.3 基于策略梯度的优化
MiniLLM的论文中提出了另一个新颖的视角——逆向KL其实可以等价于强化学习，并进行了公式推导。策略梯度是一种强化学习算法：将期望的回报写成一个可导的函数，然后求使得这个函数的最大的策略（比如使用梯度上升）。

<!-- 虽然直观上使用逆向KL散度就能更好地蒸馏模型，但实际在最优化损失函数时会遇到对短生成的偏爱以及reward hacking等问题。 -->

由于这部分涉及较多数学公式推导和强化学习，有兴趣的同学可以查看论文自行学习。



## 参考资料
- MiniLLM: Knowledge Distillation of Large Language Models
- https://github.com/microsoft/LMOps/tree/main/minillm 
- https://blog.csdn.net/ningmengzhihe/article/details/130679350