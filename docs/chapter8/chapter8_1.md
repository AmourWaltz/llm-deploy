# 8.1 并行

## 8.1.1 概述

如今的大语言模型虽然与传统预训练模型采用了相似的网络架构以及训练方法，但其通过扩展模型参数规模、训练数据量以及算力资源等方法，实现了模型性能的质变。为了提高大语言模型的推理效果，科研人员需要使用更大规模的模型并提供更多的训练数据，因此用于训练算力资源也会随之增加。当前主流的大模型如 ChatGPT 等都是在成百上千块 GPU 上训练的。为了进一步高效且经济地加速大模型训练推理，大模型的并行方法就成了人工智能领域越来越关注的重点。本文将介绍几种大模型部署中常用的并行方法。
